%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"
\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros

% Header and footer
\newcommand{\unitName}{Introductory Computational Mathematics}
\newcommand{\unitTime}{Semester 2, 2022}
\newcommand{\unitCoordinator}{Dr Elliot Carr}
\newcommand{\documentAuthors}{Tarang Janawalkar}

\fancyhead[L]{\unitName}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\unitName}} \\[0.1in]
        \normalsize{\unitTime} \\[0.2in]
        \normalsize\textit{\unitCoordinator} \\[0.2in]
        \documentAuthors
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
\newpage
%
\tableofcontents
\newpage
%
\section{Preliminaries}
\subsection{Errors}
Errors in calculations are a common problem in numerical analysis.
We can quantify the magnitude of such an error by two measures.
\begin{definition}[Absolute and relative error]
    Let \(\tilde{x}\) be an approximatiotn of \(x\). Then the \textbf{absolute
        error} is given by
    \begin{equation*}
        \text{absolute error} = \abs*{\tilde{x}-x}.
    \end{equation*}
    The \textbf{relative error} is given by
    \begin{equation*}
        \text{relative error} = \frac{\abs*{\tilde{x}-x}}{\abs*{x}}.
    \end{equation*}
\end{definition}
It is important to realise that the absolute error can be
misleading when comparing different sizes of errors,
i.e., it is always small for small values of \(x\) and \(\tilde{x}\).
\subsection{Floating Point Arithmetic}
The set of real numbers \(\R\) contains uncountably many elements.
Computers have a limited number of bits, and can
therefore only represent a small subset of these elements.

The most common approximation of real arithmetic used in computers is known
as \textbf{floating point arithmetic}.
\begin{definition}[Floating point number system]
    A floating point number system \(\mathbb{F}\left( \beta,\: k,\: m,\: M \right)\)
    is a \textit{finite subset} of the real number system characterised by the parameters:
    \begin{itemize}
        \item \(\beta \in \N\): the base
        \item \(k \in \N\): the number of digits in the significand
        \item \(m \in \Z\): the minimum exponent
        \item \(M \in \Z\): the maximum exponent
    \end{itemize}
\end{definition}
\begin{definition}[Floating point numbers]
    The floating point numbers \(f \in \mathbb{F}\left( \beta,\: k,\: m,\: M \right)\)
    are real numbers expressible in the form
    \begin{equation*}
        f = \pm \left( d_1.d_2 d_3 \dots d_k \right)_\beta \times \beta^e
    \end{equation*}
    where \(e \in \Z\) is the \textbf{exponent} satisfying \(m \leq e \leq M\).
    The quantity \(d_1.d_2 d_3 \dots d_k\) is known as the \textbf{significand},
    where \(d_i\) are base-\(\beta\) digits, with \(d_1 \neq 0\) unless \(f = 0\),
    to ensure a unique representation of \(f\).
\end{definition}
Computers primarily use floating point number systems with base \(\beta = 2\) (binary),
other common bases include \(\beta = 10\)
(decimal\footnote{Note that for base-10, we do not need to include the subscript in the significand.})
and \(\beta = 16\) (hexadecimal).

To illustrate the finiteness of the floating point number system, consider
the following example:
\begin{align*}
    \mathbb{F}\left( 10,\: 3,\: -1,\: 1\right) & = \begin{aligned}[t]
                                                       \left\{\right. & 0,                                                                                                                \\
                                                                      & \pm 1.00 \times 10^{-1},\: &  & \pm 1.01 \times 10^{-1},\: &  & \dots,\: &  & \pm 9.99 \times 10^{-1},            \\
                                                                      & \pm 1.00 \times 10^0,\:    &  & \pm 1.01 \times 10^0,\:    &  & \dots,\: &  & \pm 9.99 \times 10^0,               \\
                                                                      & \pm 1.00 \times 10^1,\:    &  & \pm 1.01 \times 10^1,\:    &  & \dots,\: &  & \pm 9.99 \times 10^1 \left.\right\}
                                                   \end{aligned} \\
                                               & = \begin{aligned}[t]
                                                       \left\{\right. & 0,                                                                        \\
                                                                      & \pm 0.100,\: &  & \pm 0.101,\: &  & \dots,\: &  & \pm 0.999,              \\
                                                                      & \pm 1.00,\:  &  & \pm 1.01,\:  &  & \dots,\: &  & \pm 9.99,               \\
                                                                      & \pm 10.0,\:  &  & \pm 10.1,\:  &  & \dots,\: &  & \pm 99.9 \left.\right\}
                                                   \end{aligned}
\end{align*}
Note that the numbers in this set are not equally spaced, (smaller spacing for smaller exponents).
\begin{definition}[Overflow and underflow]
    Consider the value \(x \in \R\), if \(x\) is too small in magnitude to be
    represented in \(\mathbb{F}\),
    an \textbf{underflow} occurs which typically causes the number to be replaced by zero.

    Similarly, if \(x\) is too large in magnitude to be represented in \(\mathbb{F}\),
    an \textbf{overflow} occurs which typically causes the number to be replaced by infinity.
\end{definition}
\begin{corollary}
    The smallest and largest values (in magnitude) of \(\mathbb{F}\) are given by
    \begin{align*}
        \min_{f \in \mathbb{F}} \abs*{f} & = \beta^m                                      \\
        \max_{f \in \mathbb{F}} \abs*{f} & = \left( 1 - \beta^{-k} \right) \beta^{M + 1}.
    \end{align*}
    The cardinality of the positive elements in \(\mathbb{F}\),
    is given by
    \begin{equation*}
        \abs*{\left\{ f \in \mathbb{F} : f > 0 \right\}} = \left( M - m + 1 \right) \left( \beta - 1 \right) \beta^{k - 1}
    \end{equation*}
    so that by including negative numbers and zero, the cardinality of \(\mathbb{F}\)
    is given by
    \begin{equation*}
        \abs*{\mathbb{F}} = 2 \abs*{\left\{ f \in \mathbb{F} : f > 0 \right\}} + 1.
    \end{equation*}
\end{corollary}
\subsubsection{Representing Real Numbers as Floating Point Numbers}
If we wish to represent a real number\footnote{\(x\) must satisfy \(\min{\left( \mathbb{F} \right)} \leq x \leq \max{\left( \mathbb{F} \right)}\).}
\(x\) that is not exactly representable in \(\mathbb{F}\),
we can \textbf{round} the number to the nearest \textit{representable} number.

The error committed by this process is known as the \textbf{roundoff error}.
\subsubsection{Converting between Floating Point Number Systems}
Consider \(fl : \R \to \mathbb{F}\left( \beta,\: k,\: m,\: M \right)\), defined as function which maps
real numbers \(x\) to the nearest element in
\(\mathbb{F}\). To determine \(fl\left( x \right)\):
\begin{enumerate}
    \item Express \(x\) in base \(\beta\).
    \item Express \(x\) in scientific form.
    \item Verify that \(m \leq e \leq M\):
          \begin{itemize}
              \item If \(e > M\), then \(x = \infty\).
              \item If \(e < m\), then \(x = 0\).
              \item Otherwise, round the significand to \(k\) digits.
          \end{itemize}
\end{enumerate}
The relative error produced by rounding \(x\) to \(fl\left( x \right)\) is bounded
according to
\begin{equation*}
    \frac{\abs*{x - fl\left( x \right)}}{\abs*{x}} \leq \frac{1}{2} \beta^{1 - k}.
\end{equation*}
\begin{definition}[Unit roundoff]
    The \textbf{unit roundoff} or \textbf{machine precision} \(u\) of a floating point number system \(\mathbb{F}\left( \beta,\: k,\: m,\: M \right)\)
    is given by
    \begin{equation*}
        u = \frac{1}{2} \beta^{1 - k}.
    \end{equation*}
\end{definition}
\subsubsection{IEEE Floating Point Standard}
IEEE 754 is the standard for floating point arithmetic used by most modern computers.

It is a binary format, with several variants. The most common variant is \textbf{IEEE double precision},
which is based on \(\mathbb{F}\left( 2,\: 53,\: -1022,\: 1023 \right)\).

The basic properties of this format are summarised in the following table.
\begin{table}[H]
    \centering
    \begin{tabular}{l | c}
        \toprule
        Unit roundoff                          & \(u = 1.11 \times 10^{-16}\)               \\
        Largest representable positive number  & \(1.80 \times 10^{308}\)                   \\
        Smallest representable positive number & \(2.23 \times 10^{-308}\)                  \\
        Special values                         & \(\pm 0\), \(\pm \infty\), \lstinline!NaN! \\
        \bottomrule
    \end{tabular}
    % \caption{} % \label{}
\end{table}
\subsection{Catastrophic Cancellation}
When working with floating point arithmetic, roundoff is a common source of error.
Certain operations may bring roundoff errors that are too large to be easily corrected.

\textbf{Catastrophic cancellation} or \textbf{cancellation error} is the error that occurs in the floating
point subtraction of two numbers that are very close to each other, where at least
one of them is not exactly representable.

As an example, the quadratic formula
\begin{align*}
    x_1 = \frac{-b + \sqrt{b^2 - 4 a c}}{2 a} &  & x_2 = \frac{-b - \sqrt{b^2 - 4 a c}}{2 a}
\end{align*}
experiences catastrophic cancellation for \(b^2 \gg 4ac\), as \(b^2 - 4ac \approx b^2\) so that
\(\sqrt{b^2 - 4ac} = \sqrt{b^2} = \abs*{b}\):
\begin{align*}
    x_1 = \frac{-b + \abs*{b}}{2 a} &  & x_2 = \frac{-b - \abs*{b}}{2 a}
\end{align*}
When \(b > 0\), \(\abs*{b} = b\), so that
\begin{equation*}
    x_1 = \frac{-b + b}{2a} = \frac{b - b}{2a}.
\end{equation*}
And when \(b < 0\), \(\abs*{b} = -b\), so that
\begin{equation*}
    x_2 = \frac{-b - \left( -b \right)}{2a} = \frac{b - b}{2a}.
\end{equation*}
This cancellation can be avoided by taking the product of the two roots to determine the exact result
of the root that suffers from catastrophic cancellation.
\begin{equation*}
    x_1 x_2 = \frac{c}{a}.
\end{equation*}
\subsection{Taylor Polynomials}
Suppose we have a function \(f\left( x \right)\) that is \(n\) differentiable at the point
\(x = x_0\). This function can be approximated by a sum of polynomials that agrees with its first \(n\) derivatives at that point.
\begin{definition}[Taylor polynomial]
    The \textbf{Taylor polynomial} of degree \(n\) of \(f\), centred at \(x_0\) is defined by
    \begin{align*}
        P_n\left( x \right) & = f\left( x_0 \right) + f'\left( x_0 \right) \left( x - x_0 \right) + \frac{f''\left( x_0 \right)}{2} \left( x - x_0 \right)^2 + \cdots + \frac{f^{\left( n \right)} \left( x_0 \right)}{n!} \left( x - x_0 \right)^n \\
                            & = \sum_{k = 0}^n \frac{f^{\left( k \right)}\left( x_0 \right)}{k!} \left( x - x_0 \right)^k.
    \end{align*}
\end{definition}
The Taylor polynomial can be used to approximate a function \(f\) for values of \(x\) near \(x_0\), the following
theorem addresses how accurate the approximation is.
\begin{definition}[Taylor's theorem]
    Suppose that \(f\) is \(n + 1\) times differentiable on an interval \(\interval{a}{b}\) containing \(x_0\),
    and let \(P_n\) be the degree \(n\) Taylor polynomial for \(f\), centred on \(x_0\). Then for all \(x \in \interval{a}{b}\),
    there exists a value \(x_0 < c < x\) such that
    \begin{equation*}
        f\left( x \right) = P_n\left( x \right) + \frac{f^{\left( n + 1 \right)}\left( c \right)}{\left( n + 1 \right)!} \left( x - x_0 \right)^{n + 1}.
    \end{equation*}
    The term
    \begin{equation*}
        R_n\left( x \right) = \frac{f^{\left( n + 1 \right)}\left( c \right)}{\left( n + 1 \right)!} \left( x - x_0 \right)^{n + 1}
    \end{equation*}
    is called the \textbf{error term} or \textbf{remainder term} for \(P_n\).
\end{definition}
To determine the absolute error from the Taylor series polynomial, consider the error term:
\begin{equation*}
    \abs*{f\left( x \right) - P_n\left( x \right)} = \abs*{R_n\left( x \right)}.
\end{equation*}
The maximum value of \(\abs*{R_n\left( x \right)}\) on the interval \(\interval{a}{b}\) gives the bound on the maximum error incurred when approximating
\(f\) by \(P_n\) on that interval.
\subsection{Taylor Series}
Given an infinitely differentiable function \(f\), we can take the limit \(n \to \infty\) to find Taylor series representation of \(f\), given by:
\begin{equation*}
    f\left( x \right) = \sum_{k = 0}^\infty \frac{f^{\left( k \right)}\left( x_0 \right)}{k!} \left( x - x_0 \right)^k.
\end{equation*}
When we truncate this series at a finite \(n\), the error from the Taylor series is known as \textbf{truncation error}. In this case,
the remainder term gives us a \textit{bound} on the size of this truncation error.
\end{document}

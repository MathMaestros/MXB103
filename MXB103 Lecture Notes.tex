%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"

\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros

% Header and footer
\newcommand{\unitName}{Introductory Computational Mathematics}
\newcommand{\unitTime}{Semester 2, 2022}
\newcommand{\unitCoordinator}{Dr Elliot Carr}
\newcommand{\documentAuthors}{Tarang Janawalkar}

\fancyhead[L]{\unitName}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\unitName}} \\[0.1in]
        \normalsize{\unitTime} \\[0.2in]
        \normalsize\textit{\unitCoordinator} \\[0.2in]
        \documentAuthors
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
\newpage
%
\tableofcontents
\newpage
%
\section{Preliminaries}
\subsection{Errors}
Errors in calculations are a common problem in numerical analysis.
We can quantify the magnitude of such an error by two measures.
\begin{definition}[Absolute and relative error]
    Let \(\tilde{x}\) be an approximation of \(x\). Then the \textbf{absolute
        error} is given by
    \begin{equation*}
        \text{absolute error} = \abs*{\tilde{x}-x}.
    \end{equation*}
    The \textbf{relative error} is given by
    \begin{equation*}
        \text{relative error} = \frac{\abs*{\tilde{x}-x}}{\abs*{x}}.
    \end{equation*}
\end{definition}
It is important to realise that the absolute error can be
misleading when comparing different sizes of errors,
i.e., it is always small for small values of \(x\) and \(\tilde{x}\).
\subsection{Floating Point Arithmetic}
The set of real numbers \(\R\) contains uncountably many elements.
Computers have a limited number of bits, and can
therefore only represent a small subset of these elements.

The most common approximation of real arithmetic used in computers is known
as \textbf{floating point arithmetic}.
\begin{definition}[Floating point number system]
    A floating point number system \(\mathbb{F}\left( \beta,\: k,\: m,\: M \right)\)
    is a \textit{finite subset} of the real number system characterised by the parameters:
    \begin{itemize}
        \item \(\beta \in \N\): the base
        \item \(k \in \N\): the number of digits in the significand
        \item \(m \in \Z\): the minimum exponent
        \item \(M \in \Z\): the maximum exponent
    \end{itemize}
\end{definition}
\begin{definition}[Floating point numbers]
    The floating point numbers \(f \in \mathbb{F}\left( \beta,\: k,\: m,\: M \right)\)
    are real numbers expressible in the form
    \begin{equation*}
        f = \pm \left( d_1.d_2 d_3 \dots d_k \right)_\beta \times \beta^e
    \end{equation*}
    where \(e \in \Z\) is the \textbf{exponent} satisfying \(m \leq e \leq M\).
    The quantity \(d_1.d_2 d_3 \dots d_k\) is known as the \textbf{significand},
    where \(d_i\) are base-\(\beta\) digits, with \(d_1 \neq 0\) unless \(f = 0\),
    to ensure a unique representation of \(f\).
\end{definition}
Computers primarily use floating point number systems with base \(\beta = 2\) (binary),
other common bases include \(\beta = 10\)
(decimal\footnote{Note that for base-10, we do not need to include the subscript in the significand.})
and \(\beta = 16\) (hexadecimal).

To illustrate the finiteness of the floating point number system, consider
the following example:
\begin{align*}
    \mathbb{F}\left( 10,\: 3,\: -1,\: 1\right) & = \begin{aligned}[t]
                                                       \left\{\right. & 0,                                                                                                                \\
                                                                      & \pm 1.00 \times 10^{-1},\: &  & \pm 1.01 \times 10^{-1},\: &  & \dots,\: &  & \pm 9.99 \times 10^{-1},            \\
                                                                      & \pm 1.00 \times 10^0,\:    &  & \pm 1.01 \times 10^0,\:    &  & \dots,\: &  & \pm 9.99 \times 10^0,               \\
                                                                      & \pm 1.00 \times 10^1,\:    &  & \pm 1.01 \times 10^1,\:    &  & \dots,\: &  & \pm 9.99 \times 10^1 \left.\right\}
                                                   \end{aligned} \\
                                               & = \begin{aligned}[t]
                                                       \left\{\right. & 0,                                                                        \\
                                                                      & \pm 0.100,\: &  & \pm 0.101,\: &  & \dots,\: &  & \pm 0.999,              \\
                                                                      & \pm 1.00,\:  &  & \pm 1.01,\:  &  & \dots,\: &  & \pm 9.99,               \\
                                                                      & \pm 10.0,\:  &  & \pm 10.1,\:  &  & \dots,\: &  & \pm 99.9 \left.\right\}
                                                   \end{aligned}
\end{align*}
Note that the numbers in this set are not equally spaced, (smaller spacing for smaller exponents).
\begin{definition}[Overflow and underflow]
    Consider the value \(x \in \R\), if \(x\) is too small in magnitude to be
    represented in \(\mathbb{F}\),
    an \textbf{underflow} occurs which typically causes the number to be replaced by zero.

    Similarly, if \(x\) is too large in magnitude to be represented in \(\mathbb{F}\),
    an \textbf{overflow} occurs which typically causes the number to be replaced by infinity.
\end{definition}
\begin{corollary}
    The smallest and largest values (in magnitude) of \(\mathbb{F}\) are given by
    \begin{align*}
        \min_{f \in \mathbb{F}} \abs*{f} & = \beta^m                                      \\
        \max_{f \in \mathbb{F}} \abs*{f} & = \left( 1 - \beta^{-k} \right) \beta^{M + 1}.
    \end{align*}
    The cardinality of the positive elements in \(\mathbb{F}\),
    is given by
    \begin{equation*}
        \abs*{\left\{ f \in \mathbb{F} : f > 0 \right\}} = \left( M - m + 1 \right) \left( \beta - 1 \right) \beta^{k - 1}
    \end{equation*}
    so that by including negative numbers and zero, the cardinality of \(\mathbb{F}\)
    is given by
    \begin{equation*}
        \abs*{\mathbb{F}} = 2 \abs*{\left\{ f \in \mathbb{F} : f > 0 \right\}} + 1.
    \end{equation*}
\end{corollary}
\subsubsection{Representing Real Numbers as Floating Point Numbers}
If we wish to represent a real number\footnote{\(x\) must satisfy \(\min{\left( \mathbb{F} \right)} \leq x \leq \max{\left( \mathbb{F} \right)}\).}
\(x\) that is not exactly representable in \(\mathbb{F}\),
we can \textbf{round} the number to the nearest \textit{representable} number.

The error committed by this process is known as the \textbf{roundoff error}.
\subsubsection{Converting between Floating Point Number Systems}
Consider \(fl : \R \to \mathbb{F}\left( \beta,\: k,\: m,\: M \right)\), defined as function which maps
real numbers \(x\) to the nearest element in
\(\mathbb{F}\). To determine \(fl\left( x \right)\):
\begin{enumerate}
    \item Express \(x\) in base \(\beta\).
    \item Express \(x\) in scientific form.
    \item Verify that \(m \leq e \leq M\):
          \begin{itemize}
              \item If \(e > M\), then \(x = \infty\).
              \item If \(e < m\), then \(x = 0\).
              \item Otherwise, round the significand to \(k\) digits.
          \end{itemize}
\end{enumerate}
The relative error produced by rounding \(x\) to \(fl\left( x \right)\) is bounded
according to
\begin{equation*}
    \frac{\abs*{x - fl\left( x \right)}}{\abs*{x}} \leq \frac{1}{2} \beta^{1 - k}.
\end{equation*}
\begin{definition}[Unit roundoff]
    The \textbf{unit roundoff} or \textbf{machine precision} \(u\) of a floating point number system \(\mathbb{F}\left( \beta,\: k,\: m,\: M \right)\)
    is given by
    \begin{equation*}
        u = \frac{1}{2} \beta^{1 - k}.
    \end{equation*}
\end{definition}
\subsubsection{IEEE Floating Point Standard}
IEEE 754 is the standard for floating point arithmetic used by most modern computers.

It is a binary format, with several variants. The most common variant is \textbf{IEEE double precision},
which is based on \(\mathbb{F}\left( 2,\: 53,\: -1022,\: 1023 \right)\).

The basic properties of this format are summarised in the following table.
\begin{table}[H]
    \centering
    \begin{tabular}{l | c}
        \toprule
        Unit roundoff                          & \(u = 1.11 \times 10^{-16}\)               \\
        Largest representable positive number  & \(1.80 \times 10^{308}\)                   \\
        Smallest representable positive number & \(2.23 \times 10^{-308}\)                  \\
        Special values                         & \(\pm 0\), \(\pm \infty\), \lstinline!NaN! \\
        \bottomrule
    \end{tabular}
    % \caption{} % \label{}
\end{table}
\subsection{Catastrophic Cancellation}
When working with floating point arithmetic, roundoff is a common source of error.
Certain operations may bring roundoff errors that are too large to be easily corrected.

\textbf{Catastrophic cancellation} or \textbf{cancellation error} is the error that occurs in the floating
point subtraction of two numbers that are very close to each other, where at least
one of them is not exactly representable.

As an example, the quadratic formula
\begin{align*}
    x_1 = \frac{-b + \sqrt{b^2 - 4 a c}}{2 a} &  & x_2 = \frac{-b - \sqrt{b^2 - 4 a c}}{2 a}
\end{align*}
experiences catastrophic cancellation for \(b^2 \gg 4ac\), as \(b^2 - 4ac \approx b^2\) so that
\(\sqrt{b^2 - 4ac} = \sqrt{b^2} = \abs*{b}\):
\begin{align*}
    x_1 = \frac{-b + \abs*{b}}{2 a} &  & x_2 = \frac{-b - \abs*{b}}{2 a}
\end{align*}
When \(b > 0\), \(\abs*{b} = b\), so that
\begin{equation*}
    x_1 = \frac{-b + b}{2a} = \frac{b - b}{2a}.
\end{equation*}
And when \(b < 0\), \(\abs*{b} = -b\), so that
\begin{equation*}
    x_2 = \frac{-b - \left( -b \right)}{2a} = \frac{b - b}{2a}.
\end{equation*}
This cancellation can be avoided by taking the product of the two roots to determine the exact result
of the root that suffers from catastrophic cancellation.
\begin{equation*}
    x_1 x_2 = \frac{c}{a}.
\end{equation*}
\subsection{Taylor Polynomials}
\begin{equation*}
    \cos{\left( x \right)} = \sum_{k = 0}^n \frac{\left( -1 \right)^k}{\left(2k \right)!} x^{2k}.
\end{equation*}
Suppose we have a function \(f\left( x \right)\) that is \(n\) differentiable at the point
\(x = x_0\). This function can be approximated by a sum of polynomials that agrees with its first \(n\) derivatives at that point.
\begin{definition}[Taylor polynomial]
    The \textbf{Taylor polynomial} of degree \(n\) of \(f\), centred at \(x_0\) is defined by
    \begin{align*}
        P_n\left( x \right) & = \sum_{k = 0}^n \frac{f^{\left( k \right)}\left( x_0 \right)}{k!} x^k.
    \end{align*}
\end{definition}
The Taylor polynomial can be used to approximate a function \(f\) for values of \(x\) near \(x_0\), the following
theorem addresses how accurate the approximation is.
\begin{definition}[Taylor's theorem]
    Suppose that \(f\) is \(n + 1\) times differentiable on an interval \(\interval{a}{b}\) containing \(x_0\),
    and let \(P_n\) be the degree \(n\) Taylor polynomial for \(f\), centred on \(x_0\). Then for all \(x \in \interval{a}{b}\),
    there exists a value \(x_0 < c < x\) such that
    \begin{equation*}
        f\left( x \right) = P_n\left( x \right) + \frac{f^{\left( n + 1 \right)}\left( c \right)}{\left( n + 1 \right)!} \left( x - x_0 \right)^{n + 1}.
    \end{equation*}
    The term
    \begin{equation*}
        R_n\left( x \right) = \frac{f^{\left( n + 1 \right)}\left( c \right)}{\left( n + 1 \right)!} \left( x - x_0 \right)^{n + 1}
    \end{equation*}
    is called the \textbf{error term} or \textbf{remainder term} for \(P_n\).
\end{definition}
To determine the absolute error from the Taylor series polynomial, consider the error term:
\begin{equation*}
    \abs*{f\left( x \right) - P_n\left( x \right)} = \abs*{R_n\left( x \right)}.
\end{equation*}
The maximum value of \(\abs*{R_n\left( x \right)}\) on the interval \(\interval{a}{b}\) gives the bound on the maximum error incurred when approximating
\(f\) by \(P_n\) on that interval.
\subsection{Taylor Series}
Given an infinitely differentiable function \(f\), we can take the limit \(n \to \infty\) to find Taylor series representation of \(f\), given by:
\begin{equation*}
    f\left( x \right) = \sum_{k = 0}^\infty \frac{f^{\left( k \right)}\left( x_0 \right)}{k!} \left( x - x_0 \right)^k.
\end{equation*}
When we truncate this series at a finite \(n\), the error from the Taylor series is known as \textbf{truncation error}. In this case,
the remainder term gives us a \textit{bound} on the size of this truncation error.
\section{Ordinary Differential Equations}
For certain classes of ordinary differential equations (ODEs), we can obtain analytical closed-form solutions using known techniques.
However for most cases we will need to use numerical techniques to obtain approximate solutions.
\subsection{Initial Value Problems}
While solutions with arbitrary constants, such as \(y = C e^{t}\) are acceptable for theoretical analysis,
we cannot have such variables when determining an approximate solution in the real world.
Hence we require \textbf{initial conditions} to obtain a definitive solution. An ODE combined
with an initial condition is called an \textbf{initial value problem} (IVP).
\subsection{Time Discretisation}
Consider the initial value problem,
\begin{align*}
    y\left( a \right)          & = \alpha                                                              \\
    \odv{y\left( t \right)}{t} & = f\left( t,\: y\left( t \right) \right), \quad \quad a \leq t \leq b
\end{align*}
Divide the interval \(\interval{a}{b}\) into \(n\) subintervals, each with width \(h = \left( b - a \right) / n\).
Then define \(t_i = a + i h\), for \(i = 0,\: 1,\: \ldots,\: n\), so that \(t_0 = a\) and \(t_n = b\). If we then compute
\(y_i = y\left( t_i \right)\) denoted as \(w_i\), then \(w_i \approx y_i\) for all \(i = 0,\: 1,\: \ldots,\: n\).
\subsection{Euler's Method}
Euler's method, or the first order Taylor method, uses a Taylor polynomial approximation of \(y\) over each subinterval.
Assuming \(y\) is twice differentiable,
\begin{equation*}
    y\left( t_i + h \right) = y\left( t_i \right) + h y'\left( t_i \right) + \mathcal{O}\left( h^2 \right).
\end{equation*}
The remainder term is not shown in its exact form but rather as \(\mathcal{O}\left( h^2 \right)\), ``Big-O of \(h^2\)'', or ``order \(h^2\)'', meaning that
the error is proportional to \(h^2\). Using the ODE and substituting \(t_{i + 1} = t_i + h\) gives
\begin{equation*}
    y_{i + 1} = y_i + h f\left( t_i,\: y_i \right) + \mathcal{O}\left( h^2 \right).
\end{equation*}
By removing the remainder term, we obtain the approximation method known as \textbf{Euler's method}:
\begin{align*}
    w_0       & = \alpha                             \\
    w_{i + 1} & = w_i + h f\left( t_i,\: w_i \right)
\end{align*}
for \(i = 0,\: 1,\: \ldots,\: n - 1\).
\subsection{Local and Global Error}
\textbf{Local error} is defined as the error that the method would incur in \textbf{one step},
assuming the solution was correct at the previous step.
The \textbf{global error} is defined as the error in the solution after \(i\) steps (at \(t = t_i\)),
and is given by:
\begin{equation*}
    \abs*{y_i - w_i}.
\end{equation*}
It is the accumulation of the local errors from steps \(1,\: 2,\: \ldots,\: i\).

The \textbf{order} of a method refers to the global error of that method. For Euler's
method, the local error is \(\mathcal{O}\left( h^2 \right)\) while the global error is
\(\mathcal{O}\left( h \right)\), hence ``\textit{first order} Taylor method''.

In general, if the local error is \(\mathcal{O}\left( h^{p + 1} \right)\), then the global error is
\(\mathcal{O}\left( h^p \right)\) and the method is said to be a \(p\)th order method.
\begin{equation*}
    \text{Global error} \approx n \mathcal{O}\left( h^{p + 1} \right) = \frac{b - a}{h} \mathcal{O}\left( h^{p + 1} \right) = \mathcal{O}\left( h^p \right).
\end{equation*}
\subsection{Second Order Taylor Method}
To improve upon the accuracy of Euler's method, we can use additional Taylor polynomial terms by truncating at a higher order.
Assuming \(y\) is three times differentiable, we have
\begin{equation*}
    y\left( t_i + h \right) = y\left( t_i \right) + h y'\left( t_i \right) + \frac{h^2}{2} y''\left( t_i \right) + \mathcal{O}\left( h^3 \right)
\end{equation*}
which can be rewritten as
\begin{equation*}
    y_{i + 1} = y_i + h f\left( t_i,\: y_i \right) + \frac{h^2}{2} f'\left( t_i\: y_i \right) + \mathcal{O}\left( h^3 \right).
\end{equation*}
Again by removing the remainder term, we obtain the approximation \(w_i\) of \(y_i\), known as the \textbf{second order Taylor method}:
\begin{align*}
    w_0       & = \alpha                                                                         \\
    w_{i + 1} & = w_i + h f\left( t_i,\: w_i \right) + \frac{h^2}{2} f'\left( t_i,\: w_i \right)
\end{align*}
for \(i = 0,\: 1,\: \ldots,\: n - 1\).
This method has a local error of \(\mathcal{O}\left( h^3 \right)\), and therefore a global error of \(\mathcal{O}\left( h^2 \right)\).
\subsection{Modified Euler Method}
Although the second order Taylor method is the more accurate than Euler's method,
we require computing \(f'\left( t,\: y \right)\).

Suppose we use a numerical approximation of the derivative of \(f\).

By definition, the derivative of a function is the limiting value of the slope of the line connecting
two nearby points on a curve,
\begin{equation*}
    f'\left( t_i,\: y_i \right) = \lim_{h \to 0} \frac{f\left( t_{i + 1},\: y_{i + 1} \right) - f\left( t_i,\: y_i \right)}{h}.
\end{equation*}
For small values of \(h\), we can approximate the derivative with
\begin{equation*}
    f'\left( t_i,\: y_i \right) \approx \frac{f\left( t_{i + 1},\: y_{i + 1} \right) - f\left( t_i,\: y_i \right)}{h}.
\end{equation*}
By deriving the error term we find that
\begin{equation*}
    f'\left( t_i,\: y_i \right) = \frac{f\left( t_{i + 1},\: y_{i + 1} \right) - f\left( t_i,\: y_i \right)}{h} + \mathcal{O}\left( h \right).
\end{equation*}
Hence the second order Taylor polynomial becomes
\begin{align*}
    y_{i + 1} & = y_i + h f\left( t_i,\: y_i \right) + \frac{h^2}{2} f'\left( t_i,\: y_i \right) + \mathcal{O}\left( h^3 \right)                                                                                                \\
              & = y_i + h f\left( t_i,\: y_i \right) + \frac{h^2}{2} \left[ \frac{f\left( t_{i + 1},\: y_{i + 1} \right) - f\left( t_i,\: y_i \right)}{h} + \mathcal{O}\left( h \right) \right] + \mathcal{O}\left( h^3 \right) \\
              & = y_i + h f\left( t_i,\: y_i \right) + \frac{h}{2} f\left( t_{i + 1},\: y_{i + 1} \right) - \frac{h}{2} f\left( t_i,\: y_i \right) + \mathcal{O}\left( h^3 \right) + \mathcal{O}\left( h^3 \right)              \\
              & = y_i + \frac{h}{2} f\left( t_i,\: y_i \right) + \frac{h}{2} f\left( t_{i + 1},\: y_{i + 1} \right) + \mathcal{O}\left( h^3 \right)                                                                             \\
              & = y_i + \frac{h}{2} \left[ f\left( t_i,\: y_i \right) + f\left( t_{i + 1},\: y_{i + 1} \right) \right] + \mathcal{O}\left( h^3 \right)                                                                          \\
              & \approx y_i + \frac{h}{2} \left[ f\left( t_i,\: y_i \right) + f\left( t_{i + 1},\: y_{i + 1} \right) \right]                                                                                                    \\
    w_{i + 1} & = w_i + \frac{h}{2} \left[ f\left( t_i,\: w_i \right) + f\left( t_{i + 1},\: w_{i + 1} \right) \right]
\end{align*}
As this formula involves itself, we will use Euler's method on
\(w_{i + 1} = w_i + h f\left( t_i,\: w_i \right)\) on the RHS\@.
\begin{align*}
    w_{i + 1} & = w_i + \frac{1}{2} \left[ h f\left( t_i,\: w_i \right) + f\left( t_{i + 1},\: w_{i + 1} \right) \right]                                                                                     \\
              & = w_i + \frac{1}{2} \left[ \underbrace{h f\left( t_i,\: w_i \right)}_{k_1} + \underbrace{h f\left( t_{i + 1},\: w_i + \underbrace{h f\left( t_i,\: w_i \right)}_{k_1} \right)}_{k_2} \right] \\
              & = w_i + \frac{1}{2} \left( k_1 + k_2 \right)
\end{align*}
This result leads to what is known as the \textbf{modified Euler method}.
\begin{align*}
    w_0       & = \alpha                                     \\
    w_{i + 1} & = w_i + \frac{1}{2} \left( k_1 + k_2 \right) \\
    k_1       & = h f\left( t_i,\: w_i \right)               \\
    k_2       & = h f\left( t_i + h,\: w_i + k_1 \right)
\end{align*}
This method has a local error of \(\mathcal{O}\left( h^3 \right)\), and therefore a global error of \(\mathcal{O}\left( h^2 \right)\).
\subsection{Runge-Kutta Method}
Another popular method that agrees with the Taylor method for high orders is the Runge-Kutta method.
It is a fourth order method, referred to as RK4. It has the following form:
\begin{align*}
    w_0       & = \alpha                                                     \\
    w_{i + 1} & = w_i + \frac{1}{6} \left( k_1 + 2 k_2 + 2 k_3 + k_4 \right) \\
    k_1       & = h f\left( t_i,\: w_i \right)                               \\
    k_2       & = h f\left( t_i + \frac{h}{2},\: w_i + \frac{k_1}{2} \right) \\
    k_3       & = h f\left( t_i + \frac{h}{2},\: w_i + \frac{k_2}{2} \right) \\
    k_4       & = h f\left( t_i + h,\: w_i + k_3 \right)
\end{align*}
for \(i = 0,\: 1,\: \ldots,\: n - 1\). The local error is \(\mathcal{O}\left( h^4 \right)\), and the global error is \(\mathcal{O}\left( h^3 \right)\).
\section{Interpolation}
\begin{definition}[Interpolating function]
    A function \(f\) is said to interpolate the data points
    \begin{equation*}
        \left\{ \left( x_0,\: y_0 \right),\: \left( x_1,\: y_1 \right),\: \dots,\: \left( x_n,\: y_n \right) \right\}
    \end{equation*}
    if it satisfies \(f\left( x_0 \right) = y_0\), \(f\left( x_1 \right) = y_1\), \dots,  \(f\left( x_n \right) = y_n\).
\end{definition}
There are many kinds of functions that may satisfy an interpolating function, however, the most common choice is a
\textbf{polynomial}.

The following theorem addresses the existence and uniqueness of interpolating polynomials, given distinct \(x\)-values, or \textbf{abscissas}.
\begin{theorem}
    Let the abscissas \(x_0\), \(x_1\), \dots, \(x_n\) be distinct and the function values \(y_0\), \(y_1\), \dots, \(y_n\)
    be given. Then there exists a \textbf{unique} interpolating polynomial \(P_n\) of degree (at most) \(n\), such that
    \(P_n\left( x_i \right) = y_i\) for \(i \in \interval{0}{n}\).
\end{theorem}
The following methods are used to find the interpolating polynomial.
\subsection{Lagrange Form}
\subsubsection{Two Data Points}
Consider constructing the interpolating polynomial through the points \(\left\{ \left( x_0,\: y_0 \right),\: \left( x_1,\: y_1 \right) \right\}\).
We require a polynomial of degree \(1\) to satisfy the interpolating function. Let \(P_1\left( x \right) = a_0 + a_1 x\) satisfy the following
conditions:
\begin{align*}
    P_1\left( x_0 \right) & = y_0 & \iff a_0 + a_1 x_0 & = y_0 \\
    P_1\left( x_1 \right) & = y_1 & \iff a_0 + a_1 x_1 & = y_1
\end{align*}
solving this linear system of equations, we obtain the following:
\begin{align*}
    a_0 & = \frac{x_1 y_0 - x_0 y_1}{x_1 - x_0} \\
    a_1 & = \frac{y_1 - y_0}{x_1 - x_0}
\end{align*}
so that
\begin{equation*}
    P_1\left( x \right) = \frac{x_1 y_0 - x_0 y_1}{x_1 - x_0} + \frac{y_1 - y_0}{x_1 - x_0} x.
\end{equation*}
To generalise this process, consider the following rearrangement of the polynomial:
\begin{align*}
    P_1\left( x \right) & = \frac{x_1 y_0 - x_0 y_1}{x_1 - x_0} + \frac{y_1 - y_0}{x_1 - x_0} x                                                              \\
                        & = \frac{x_1 y_0}{x_1 - x_0} - \frac{x_0 y_1}{x_1 - x_0} + \frac{y_1 x}{x_1 - x_0} - \frac{y_0 x}{x_1 - x_0}                        \\
                        & = \left( \frac{-x}{x_1 - x_0} + \frac{x_1}{x_1 - x_0} \right) y_0 + \left( \frac{x}{x_1 - x_0} - \frac{x_0}{x_1 - x_0} \right) y_1 \\
                        & = \frac{x - x_1}{x_0 - x_1} y_0 + \frac{x - x_0}{x_1 - x_0} y_1.
\end{align*}
This form clearly shows that each point satisfies the interpolating function.
\begin{itemize}
    \item The coefficient of \(y_0\) is \textbf{1} when \(x = x_0\), and \textbf{0} when \(x = x_1\).
    \item The coefficient of \(y_1\) is \textbf{0} when \(x = x_0\), and \textbf{1} when \(x = x_1\).
\end{itemize}
we can then write \(P_1\left( x \right)\) as
\begin{equation*}
    P_1\left( x \right) = L_{1,\: 0}\left( x \right) y_0 + L_{1,\: 1}\left( x \right) y_1
\end{equation*}
where \(L_{1,\: 0}\left( x \right) = \frac{x - x_1}{x_0 - x_1}\) and \(L_{1,\: 1}\left( x \right) = \frac{x - x_0}{x_1 - x_0}\)
are the Lagrange basis functions. The first index corresponds to the degree
of the polynomial, and the second index corresponds to the index of the
point for which it is the coefficient of.
\subsubsection{Three Data Points}
For three data points, consider the reverse problem:
\begin{itemize}
    \item The coefficient of \(y_0\) is \textbf{1} when \(x = x_0\), \textbf{0} when \(x = x_1\), and \textbf{0} when \(x = x_2\).
    \item The coefficient of \(y_1\) is \textbf{0} when \(x = x_0\), \textbf{1} when \(x = x_1\), and \textbf{0} when \(x = x_2\).
    \item The coefficient of \(y_2\) is \textbf{0} when \(x = x_0\), \textbf{0} when \(x = x_1\), and \textbf{1} when \(x = x_2\).
\end{itemize}
then solve for the Lagrange basis functions:
\begin{align*}
    P_2\left( x \right) & = L_{2,\: 0}\left( x \right) y_0 + L_{2,\: 1}\left( x \right) y_1 + L_{2,\: 2}\left( x \right) y_2                                                                                                                                                                                                                                   \\
                        & = \frac{\left( x - x_1 \right)\left( x - x_2 \right)}{\left( x_0 - x_1 \right)\left( x_0 - x_2 \right)} y_0 + \frac{\left( x - x_0 \right)\left( x - x_2 \right)}{\left( x_1 - x_0 \right)\left( x_1 - x_2 \right)} y_1 + \frac{\left( x - x_0 \right)\left( x - x_1 \right)}{\left( x_2 - x_0 \right)\left( x_2 - x_1 \right)} y_2.
\end{align*}
\subsubsection{Arbitrary Number of Points}
Given the abscissas \(\left\{ x_0,\: x_1,\: \dots,\: x_n \right\}\) and function values
\(\left\{ y_0,\: y_1,\: \dots,\: y_n \right\}\) the polynomial \(P_n\left( x \right)\) defined by:
\begin{equation*}
    P_n\left( x \right) = \sum_{i = 0}^n L_{n,\: i}\left( x \right) y_i
\end{equation*}
where
\begin{equation*}
    L_{n,\: i}\left( x \right) = \prod_{j = 0,\: j \neq i}^n \frac{x - x_j}{x_i - x_j}
\end{equation*}
is called the \textbf{Lagrange form} of the interpolating polynomial.

This arises from the fact that we can construct the Lagrange form using
\begin{align*}
    P_n\left( x \right) & = L_{n,\: 0}\left( x \right) y_0 + L_{n,\: 1}\left( x \right) y_1 + \cdots + L_{n,\: n}\left( x \right) y_n \\
                        & = \sum_{i = 0}^n L_{n,\: i}\left( x \right) y_i
\end{align*}
where the Lagrange basis function is as follows:
\begin{align*}
    L_{n,\: i}\left( x \right) & = \frac{\left( x - x_0 \right)\left( x - x_1 \right) \cdots \left( x - x_{i - 1} \right) \left( x - x_{i + 1} \right) \cdots \left( x - x_n \right)}{\left( x_i - x_0 \right)\left( x_i - x_1 \right) \cdots \left( x_i - x_{i - 1} \right) \left( x_i - x_{i + 1} \right) \cdots \left( x_i - x_n \right)} \\
                               & = \prod_{j = 0,\: j \neq i}^n \frac{x - x_j}{x_i - x_j}
\end{align*}
Note that \(L_{n,\: i}\left( x \right)\) is equal to \textbf{1} when \(x = x_i\), and \textbf{0} for all other abscissa:
\begin{equation*}
    L_{n,\: i}\left( x_j \right) = \delta_{ij}
\end{equation*}
\section{Error in Polynomial Approximation}
\begin{theorem}[Lagrange form of remainder]
    Suppose that \(f\) is \(n + 1\) times continuously \linebreak differentiable on \(\interval{a}{b}\)\footnote{\(a = \min \left( x_i \right)\) and \(b = \max \left( x_i \right)\).}
    with distinct abscissas \(x_0,\: x_1,\: \dots,\: x_n\) and interpolating polynomial
    \(P_n\left( x \right)\).

    Then for all \(x \in \interval{a}{b}\) there exists a \(c \in \interval{a}{b}\) such that
    \begin{equation*}
        f\left( x \right) = P_n\left( x \right) + \frac{f^{\left( n + 1 \right)} \left( c \right)}{\left( n + 1 \right)!} \left( x - x_0 \right) \left( x - x_1 \right) \cdots \left( x - x_n \right).
    \end{equation*}
    where the term
    \begin{equation*}
        R_n\left( x \right) = \frac{f^{\left( n + 1 \right)} \left( c \right)}{\left( n + 1 \right)!} \left( x - x_0 \right) \left( x - x_1 \right) \cdots \left( x - x_n \right)
    \end{equation*}
    is called the Lagrange form of the remainder term for \(P_n\left( x \right)\).
\end{theorem}
\end{document}
